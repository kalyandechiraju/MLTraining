{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning Series\n",
    "\n",
    "## Linear Regression using Gradient Descent\n",
    "\n",
    "\n",
    "### Gradient Descent Overview\n",
    "\n",
    "<img src=\"../Assets/gd.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, we have a data set is a `collection of test scores and the amount of hours studied`.\n",
    "\n",
    "X values are the amount of hours studied.\n",
    "Y values are test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "points = np.genfromtxt(\"../Datasets/udacity_student_data.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "# Choosing Learning rate => it is a balance between choosing high value or low value.\n",
    "# Its like a bell curve. If the value is too low, our model will take too long to converge.\n",
    "# If the value is too high, model will never converge. We have to guess and check for the optimal value.\n",
    "learning_rate = 0.0001 \n",
    "\n",
    "# y = mx + b\n",
    "# m => Slope\n",
    "# b => y-intercept\n",
    "initial_b = 0\n",
    "initial_m = 0\n",
    "\n",
    "# Depends on how large the data set is. Bigger the data set, Higher the number of iterations.\n",
    "num_iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum of Squared Errors\n",
    "<img src=\"../Assets/sose.JPG\"/>\n",
    "\n",
    "#### Formula\n",
    "\n",
    "<img src=\"../Assets/gd_formula.png\"/>\n",
    "\n",
    "#### Gradient Descent Partial Derivative Formula\n",
    "\n",
    "<img src=\"../Assets/gd_partialderivative.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Every time-step our goal is to improve model prediction (accuracy).\n",
    "# For each time-step (iteration) we compute error and try to reduce it in the next time-step.\n",
    "# We want to measure the distance from each data point to the line and then square them and sum them all together\n",
    "# and divide by total number of points. This is error value and our goal is to minimize this.\n",
    "# \"SUM OF SQUARED ERRORS\"\n",
    "def compute_error_for_given_points(b, m, points):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_error += (y - (m * x + b)) **2\n",
    "    return total_error / float(len(points))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current, points, learning_rate):\n",
    "    # Gradient Descent Logic\n",
    "    # Gradient => slope => it gives us the way or the direction to move our point towards optimal value\n",
    "    # To calculate this gradient we do partial detivative\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    \n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # As per the partial derivative formula\n",
    "        b_gradient += (y - ((m_current * x) + b_current))\n",
    "        m_gradient += (x * (y - ((m_current * x) + b_current)))\n",
    "    b_gradient = -(2/N) * b_gradient\n",
    "    m_gradient = -(2/N) * m_gradient\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, np.array(points), learning_rate)\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0889365199374\n",
      "1.47774408519\n"
     ]
    }
   ],
   "source": [
    "[b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "print(b)\n",
    "print(m)\n",
    "#0.0889365199374\n",
    "# 1.47774408519"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
